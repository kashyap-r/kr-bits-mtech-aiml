{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30486a2b-2d24-4e4c-82a3-adea0e453333",
   "metadata": {},
   "source": [
    "**Spam Mail Detection using Rocchio and KNN Classifiers**\n",
    "\n",
    "This notebook implements spam mail detection using two classification algorithms: Rocchio and K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ca1a0-0edf-4ad6-af5d-08b3ea94c772",
   "metadata": {},
   "source": [
    "**1. Preprocess the Dataset**\n",
    "\n",
    "**Step 1:** \n",
    "\n",
    "Collect Dataset dataset with a minimum of 50 files of email text. \n",
    "Split the dataset into two directories: spam and non_spam.\n",
    ".\n",
    "\n",
    "**Step 2:** \n",
    "\n",
    ": Preprocessing the Dataset Tokenization: Splitting emails into words. Stop-word removal: Eliminate common words like \"and,\" \"the,\" etc. Lowercasing: Standardize the text by converting everything to lowercase. Stemming/Lemmatization: Reduce words to their root forms (e.g., \"running\" â†’ \"run\"). Built-in libraries like nltk or re can be used for preprocessin\n",
    "\n",
    "Split the Dataset Use 25% of the data for training and the rest for testingg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5caa31b9-119f-476c-a4f6-08928362d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will load and preprocess the dataset. The preprocessing includes reading files and cleaning the text.\n",
    "\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to datasets\n",
    "spam_path = 'C:\\\\spam'\n",
    "non_spam_path = 'C:\\\\non_spam'\n",
    "\n",
    "# Load the dataset (Preprocessing: reading files, cleaning text)\n",
    "def load_data(spam_dir, non_spam_dir):\n",
    "    emails, labels = [], []\n",
    "    \n",
    "    # Reading spam emails\n",
    "    for filename in os.listdir(spam_dir):\n",
    "        with open(os.path.join(spam_dir, filename), 'r', encoding='latin-1') as file:\n",
    "            emails.append(file.read())\n",
    "            labels.append(1)  # 1 represents spam\n",
    "    \n",
    "    # Reading non-spam emails\n",
    "    for filename in os.listdir(non_spam_dir):\n",
    "        with open(os.path.join(non_spam_dir, filename), 'r', encoding='latin-1') as file:\n",
    "            emails.append(file.read())\n",
    "            labels.append(0)  # 0 represents non-spam\n",
    "            \n",
    "    return emails, labels\n",
    "\n",
    "emails, labels = load_data(spam_path, non_spam_path)\n",
    "\n",
    "# Cleaning text (remove non-alphanumeric characters, converting to lowercase)\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\W+', ' ', text).lower()\n",
    "\n",
    "emails = [clean_text(email) for email in emails]\n",
    "\n",
    "# Splitting the dataset (25% for training)\n",
    "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.75, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c592e11-780e-46a0-8787-16c160d1d1fd",
   "metadata": {},
   "source": [
    "**2. Compute TF-IDF with Length Normalization Manually**\n",
    "\n",
    "We will manually compute tf-idf and normalize the vectors.\n",
    "\n",
    "**Step 2.1:** \n",
    "\n",
    "Calculate Term Frequency (TF) For each document, calculate the number of times each word appears (term frequency).\n",
    "\n",
    "**Step 2.2**\n",
    "\n",
    "Calculate Inverse Document Frequency (IDF) IDF is calculateds.\n",
    "\n",
    "IDF = log(N/( 1+Df(t))\n",
    "\n",
    " where N is the total number of documents, and DF(t) is the number of documents containing ter\n",
    "\n",
    "**Step 2.3**\n",
    "\n",
    "Combine TF and IDF For each word in each document, multiply the term frequency by the inverse document frequency to get the TF-IDF value.\r\n",
    "\r\n",
    "Normalization:normalize the TF-IDF vectors by dividing each component by the length of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a768ad4-15be-43ee-a97b-31319df0587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample TF-IDF Vector: {'subject': -0.0026971221027534283, 'hourly': 0.30425728751812664, 'gas': 0.08723466314477253, 'deals': 0.17759812356848592, 'fyi': 0.074781417473271, 'if': 0.04301212148212375, 'you': 0.004355226643817182, 'have': 0.011055635415662193, 'any': 0.012930643692439526, 'comments': 0.0858370528889332, 'please': 0.02980369475968716, 'get': 0.02663767836610456, 'them': 0.0359055474186873, 'to': -0.005394244205506857, 'tommy': 0.30425728751812664, 'forwarded': 0.05327535673220912, 'by': 0.007559585750886599, 'brenda': 0.1716741057778664, 'f': 0.05327535673220912, 'herod': 0.1716741057778664, 'hou': 0.19257495642866576, 'ect': 0.33700617375016506, 'on': 0.013065679931451548, '07': 0.149562834946542, '17': 0.1324120008492973, '2000': 0.07913664411708816, '04': 0.04814373910716644, '54': 0.074781417473271, 'pm': 0.03956832205854408, 'enron': 0.11870496617563225, 'technology': 0.05919937452282864, 'from': 0.004355226643817182, 'j': 0.043617331572386266, 'yanowski': 0.10141909583937556, '11': 0.02948565118047909, '08': 0.04814373910716644, 'am': 0.03256169615672407, 'robert': 0.05919937452282864, 'superty': 0.10141909583937556, 'edward': 0.074781417473271, 'd': 0.07913664411708816, 'gottlob': 0.10141909583937556, 'donna': 0.0858370528889332, 'greif': 0.10141909583937556, 'cc': 0.03956832205854408, 'scott': 0.0858370528889332, 'mills': 0.0858370528889332, 'dave': 0.074781417473271, 'nommensen': 0.10141909583937556, 'jeff': 0.0858370528889332, 'johnson': 0.074781417473271, 'corp': 0.03956832205854408, 'romeo': 0.0858370528889332, 'souza': 0.0858370528889332, 'here': 0.0359055474186873, 'is': 0.0013976102558393368, 'a': -0.004045683154130142, 'high': 0.05327535673220912, 'level': 0.06620600042464865, 'summary': 0.06620600042464865, 'of': 0.0013976102558393368, 'our': 0.03256169615672407, 'discussions': 0.06620600042464865, 'last': 0.043617331572386266, 'week': 0.05327535673220912, 'i': 0.01490184737984358, 'believe': 0.05327535673220912, 'that': 0.004355226643817182, 'the': 0.0, 'calculations': 0.10141909583937556, 'listed': 0.05919937452282864, 'below': 0.043617331572386266, 'under': 0.09628747821433288, 'option': 0.149562834946542, '1': 0.01917625370684967, 'show': 0.06620600042464865, 'how': 0.043617331572386266, 'hpl': 0.05919937452282864, 'would': 0.07913664411708816, 'like': 0.0718110948373746, 'capture': 0.10141909583937556, 'and': -0.0026971221027534283, 'see': 0.08845695354143727, 'their': 0.023986279108101725, 'postions': 0.10141909583937556, 'all': 0.007559585750886599, 'based': 0.03956832205854408, '24': 0.1716741057778664, 'hour': 0.1716741057778664, 'flow': 0.22434425241981298, 'rate': 0.149562834946542, 'basis': 0.15982607019662737, '2': 0.023986279108101725, 'shows': 0.06620600042464865, 'what': 0.03256169615672407, 'values': 0.0858370528889332, 'look': 0.03956832205854408, 'an': 0.00926786905258274, 'bob': 0.05919937452282864, 'review': 0.043617331572386266, 'with': 0.007559585750886599, 'other': 0.021506060741061876, 'trading': 0.05327535673220912, 'directors': 0.10141909583937556, 'they': 0.02663767836610456, 'also': 0.03956832205854408, 'prefer': 0.10141909583937556, 'everything': 0.074781417473271, 'thanks': 0.04814373910716644}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize and calculate term frequencies\n",
    "def compute_tf(email):\n",
    "    words = email.split()\n",
    "    tf = Counter(words)\n",
    "    return tf\n",
    "\n",
    "# Compute inverse document frequency\n",
    "def compute_idf(emails):\n",
    "    N = len(emails)\n",
    "    idf = Counter()\n",
    "    for email in emails:\n",
    "        unique_words = set(email.split())\n",
    "        for word in unique_words:\n",
    "            idf[word] += 1\n",
    "    idf = {word: math.log(N / (1 + count)) for word, count in idf.items()}\n",
    "    return idf\n",
    "\n",
    "# Compute tf-idf for each email\n",
    "def compute_tfidf(email, idf):\n",
    "    tf = compute_tf(email)\n",
    "    tfidf = {word: tf[word] * idf.get(word, 0) for word in tf}\n",
    "    return tfidf\n",
    "\n",
    "idf = compute_idf(X_train)\n",
    "train_tfidf = [compute_tfidf(email, idf) for email in X_train]\n",
    "\n",
    "# Length normalization\n",
    "def normalize_vector(tfidf):\n",
    "    norm = math.sqrt(sum([value**2 for value in tfidf.values()]))\n",
    "    if norm == 0:\n",
    "        return tfidf\n",
    "    return {word: value / norm for word, value in tfidf.items()}\n",
    "\n",
    "normalized_train_tfidf = [normalize_vector(tfidf) for tfidf in train_tfidf]\n",
    "\n",
    "# Display a sample of computed tf-idf\n",
    "print(\"Sample TF-IDF Vector:\", normalized_train_tfidf[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7f3fd-d5dc-4790-8164-ffa94225d0dc",
   "metadata": {},
   "source": [
    "**3. Implement and Train Classifiers**\n",
    "   \n",
    "**Rocchio Classifier**\n",
    "\n",
    "**Step 3.1**\n",
    "\n",
    "Compute Centroid for Each Class The centroid for each class (spam and non-spam) is computed as the average of the TF-IDF vectors for the documents in that class.\n",
    "\n",
    "**Step 3.2**\n",
    "\n",
    " Classify New Emails Using cosine similarity, classify a new email based on its distance from the centroids of the spam and non-spam classe\n",
    "\n",
    "compute the centroids for both spam and non_spam.\n",
    "\n",
    "\r\n",
    "\r\n",
    "Cosine Similarity Formula:\r\n",
    "\r\n",
    "Cosine Similarity (ð´,ðµ)=ð´â‹…ðµ / âˆ¥ð´âˆ¥âˆ¥ðµâˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f57e2f0-e3b0-4634-a5f0-1dffd1f6c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centroid for each class\n",
    "def compute_centroids(tfidf_data, labels):\n",
    "    spam_centroid, non_spam_centroid = {}, {}\n",
    "    spam_count, non_spam_count = 0, 0\n",
    "\n",
    "    for i, tfidf in enumerate(tfidf_data):\n",
    "        if labels[i] == 1:  # Spam\n",
    "            spam_count += 1\n",
    "            for word, value in tfidf.items():\n",
    "                spam_centroid[word] = spam_centroid.get(word, 0) + value\n",
    "        else:  # Non-spam\n",
    "            non_spam_count += 1\n",
    "            for word, value in tfidf.items():\n",
    "                non_spam_centroid[word] = non_spam_centroid.get(word, 0) + value\n",
    "\n",
    "    # Average the values\n",
    "    for word in spam_centroid:\n",
    "        spam_centroid[word] /= spam_count\n",
    "    for word in non_spam_centroid:\n",
    "        non_spam_centroid[word] /= non_spam_count\n",
    "    \n",
    "    return normalize_vector(spam_centroid), normalize_vector(non_spam_centroid)\n",
    "\n",
    "spam_centroid, non_spam_centroid = compute_centroids(normalized_train_tfidf, y_train)\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum([vec1.get(word, 0) * vec2.get(word, 0) for word in vec1])\n",
    "    norm1 = math.sqrt(sum([value**2 for value in vec1.values()]))\n",
    "    norm2 = math.sqrt(sum([value**2 for value in vec2.values()]))\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Classify using Rocchio\n",
    "def classify_rocchio(email, spam_centroid, non_spam_centroid):\n",
    "    email_tfidf = normalize_vector(compute_tfidf(email, idf))\n",
    "    spam_similarity = cosine_similarity(email_tfidf, spam_centroid)\n",
    "    non_spam_similarity = cosine_similarity(email_tfidf, non_spam_centroid)\n",
    "    \n",
    "    return 1 if spam_similarity > non_spam_similarity else 0\n",
    "\n",
    "# Testing Rocchio classifier\n",
    "rocchio_predictions = [classify_rocchio(email, spam_centroid, non_spam_centroid) for email in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8888e6-4506-4f55-8d6c-fe305158bb27",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors (KNN) Classifier**\n",
    "\n",
    "**Step 4.1:**\n",
    "\n",
    "Calculate Cosine Similarity For a new email, calculate the cosine similarity between its vector and the vectors of all the training emails\n",
    "\n",
    "**Step 4.2**\n",
    "\n",
    ": Determine the Majority Class Select the top k emails with the highest similarity and classify the email as the majority class among the nearest neighbor\n",
    "\n",
    "store the training vectors and their corresponding labelss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eab788a-6bb0-4cb1-a5e7-246a8c8f5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify based on k-nearest neighbors using cosine similarity\n",
    "def classify_knn(test_email, train_data, labels, k=3):\n",
    "    email_tfidf = normalize_vector(compute_tfidf(test_email, idf))\n",
    "    \n",
    "    similarities = []\n",
    "    for i, train_email in enumerate(train_data):\n",
    "        similarity = cosine_similarity(email_tfidf, train_email)\n",
    "        similarities.append((similarity, labels[i]))\n",
    "    \n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # Get the top k neighbors\n",
    "    top_k = [label for _, label in similarities[:k]]\n",
    "    \n",
    "    # Return the most common class\n",
    "    return 1 if sum(top_k) > len(top_k) / 2 else 0\n",
    "\n",
    "# Testing KNN classifier\n",
    "knn_predictions = [classify_knn(email, normalized_train_tfidf, y_train, k=3) for email in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fcfd5-c4ef-44a3-9c0c-2d3a4f237111",
   "metadata": {},
   "source": [
    "**4. Evaluate Classifiers**\n",
    "\n",
    " Calculate Accuracy ,Compare the predicted labels with the actual labels for both Rocchio and KNN to evaluate the classifiers' performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51528354-fcde-442a-a14d-28b61c97f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rocchio Classifier Accuracy: 0.89\n",
      "KNN Classifier Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Accuracy evaluation\n",
    "def evaluate(predictions, true_labels):\n",
    "    correct = sum([1 for i in range(len(predictions)) if predictions[i] == true_labels[i]])\n",
    "    return correct / len(true_labels)\n",
    "\n",
    "rocchio_accuracy = evaluate(rocchio_predictions, y_test)\n",
    "knn_accuracy = evaluate(knn_predictions, y_test)\n",
    "\n",
    "print(f\"Rocchio Classifier Accuracy: {rocchio_accuracy:.2f}\")\n",
    "print(f\"KNN Classifier Accuracy: {knn_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf53821-a71a-411e-b56c-bc6a33826989",
   "metadata": {},
   "source": [
    "**5. Discussion**\n",
    "\n",
    "\n",
    "**Rocchio vs KNN Performance**\n",
    "\n",
    "**Rocchio Classifier:** May perform well if the spam and non-spam emails are well-separated in the feature space. It is based on centroid similarity which works best when classes are distinct.\n",
    "\n",
    "**KNN Classifier:** Generally robust to noise and can adapt better to variations in data. Performance depends on the choice of 'k'.\n",
    "\n",
    "**Effect of 'k' in KNN**\n",
    "\n",
    "**Small 'k':** The classifier is sensitive to noise and outliers.\n",
    "\n",
    "**Large 'k':** The classifier may smooth out distinctions between classes but can be less sensitive to local variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cb766-aa89-4467-b1bb-70a8c1c3f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
