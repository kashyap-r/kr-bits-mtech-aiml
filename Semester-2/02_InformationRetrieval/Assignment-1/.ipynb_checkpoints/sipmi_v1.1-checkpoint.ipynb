{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af39735-f967-48e5-98e8-06778c46d71e",
   "metadata": {},
   "source": [
    "To construct a positional index for preprocessed documents and implement the SPIMI (Single-Pass In-Memory Indexing) algorithm, we'll need to follow these steps:\n",
    "\n",
    "Read and preprocess the documents (tokenization, stop word removal, and stemming).\n",
    "Construct the positional index using the SPIMI algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514d2d4-5d21-4ec9-b728-8c943d6fa917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required libraries \n",
    "!pip install python-docx nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f5d99f-a637-41c1-8ff6-9d57a2bcc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\usha_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usha_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK data\n",
    "# -- punctuation library\n",
    "nltk.download('punkt')\n",
    "# -- stop word library\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749f7489-ec9e-4e0b-b309-e4f365b60df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the document\n",
    "def preprocess(text):    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    for each in range(len(tokens)):\n",
    "        tokens[each] = re.sub(r'\\d+', '', tokens[each])  # Remove numbers\n",
    "        tokens[each] = re.sub(r'[^\\w\\s]', '', tokens[each])  # Remove special characters\n",
    "    \n",
    "    # Remove stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106469ed-433f-4bb7-9d1a-c587d9373a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DOCX files from directory\n",
    "def read_multiple_docx(directory):\n",
    "    documents_content = {}\n",
    "    for filename in os.listdir(directory[\"docx\"]):\n",
    "        if filename.endswith('.docx'):\n",
    "            file_path = os.path.join(directory[\"docx\"], filename)\n",
    "            doc = Document(file_path)\n",
    "            content = []\n",
    "            for para in doc.paragraphs:\n",
    "                content.append(para.text)\n",
    "            documents_content[filename] = preprocess('\\n'.join(content))\n",
    "    return documents_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74b274a3-2cb6-4607-b004-0828abc66565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kashyap\n",
    "# SPIMI Algorithm for Index Construction\n",
    "def spimi_invert(token_stream, block_size=100000):\n",
    "    dictionary = defaultdict(list)\n",
    "    block_id = 0\n",
    "\n",
    "    for token, doc_id, pos in token_stream:\n",
    "        dictionary[token].append((doc_id, pos))\n",
    "        if len(dictionary) >= block_size:\n",
    "            write_block_to_disk(dictionary, block_id)\n",
    "            dictionary = defaultdict(list)\n",
    "            block_id += 1\n",
    "    \n",
    "    if dictionary:\n",
    "        write_block_to_disk(dictionary, block_id)\n",
    "\n",
    "    return block_id\n",
    "\n",
    "def write_block_to_disk(dictionary, block_id):\n",
    "    with open(f'block_{block_id}.txt', 'w') as file:\n",
    "        for term, postings in sorted(dictionary.items()):\n",
    "            file.write(f'{term}: {postings}\\n')\n",
    "\n",
    "def merge_blocks(num_blocks):\n",
    "    final_index = defaultdict(list)\n",
    "\n",
    "    for block_id in range(num_blocks+1):\n",
    "        with open(f'block_{block_id}.txt', 'r') as file:\n",
    "            for line in file:\n",
    "                term, postings = line.strip().split(': ')\n",
    "                postings = eval(postings)\n",
    "                final_index[term].extend(postings)\n",
    "\n",
    "    return final_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d7e1a2-c74d-4c6d-b54e-b77eb1b47e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./BBC Sport/text'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5576fd7d-856b-4298-a4df-5d177049bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read dataset\n",
      "2. Pre-processing Complete.\n",
      "<class 'int'>\n",
      "0\n",
      ": [(0, 18), (0, 28), (0, 29), (0, 30), (0, 31), (1, 21), (1, 23), (2, 28), (2, 29), (2, 30), (2, 31), (2, 32), (2, 33), (3, 27), (5, 15), (5, 31)]\n",
      "aaa: [(5, 7)]\n",
      "achiev: [(4, 14)]\n",
      "aliv: [(2, 12)]\n",
      "although: [(5, 23)]\n",
      "andi: [(2, 15)]\n",
      "athlet: [(3, 19), (5, 18)]\n",
      "aussi: [(0, 3)]\n",
      "australia: [(1, 30)]\n",
      "australian: [(0, 15), (1, 4), (1, 9), (2, 9)]\n",
      "award: [(4, 28)]\n",
      "back: [(2, 2)]\n",
      "battl: [(0, 2)]\n",
      "beat: [(0, 10), (4, 30)]\n",
      "believ: [(3, 28)]\n",
      "boss: [(3, 12), (3, 21)]\n",
      "britain: [(4, 16)]\n",
      "campbel: [(4, 23)]\n",
      "career: [(3, 5)]\n",
      "challeng: [(5, 26)]\n",
      "champion: [(0, 19)]\n",
      "claim: [(0, 20)]\n",
      "clear: [(3, 14)]\n",
      "close: [(0, 34)]\n",
      "come: [(2, 27)]\n",
      "comment: [(1, 34)]\n",
      "countri: [(3, 25)]\n",
      "court: [(0, 39), (1, 2), (1, 11)]\n",
      "critic: [(1, 14)]\n",
      "darren: [(4, 22)]\n",
      "davenport: [(0, 12)]\n",
      "defend: [(1, 8)]\n",
      "devonish: [(4, 25)]\n",
      "domin: [(5, 19)]\n",
      "dream: [(2, 8)]\n",
      "drug: [(3, 16)]\n",
      "face: [(2, 22)]\n",
      "faster: [(1, 28)]\n",
      "favourit: [(2, 21), (4, 32)]\n",
      "feder: [(3, 20)]\n",
      "fight: [(2, 1)]\n",
      "final: [(2, 4), (2, 26), (5, 30)]\n",
      "finish: [(5, 22)]\n",
      "first: [(0, 25)]\n",
      "five: [(0, 41)]\n",
      "fourset: [(2, 13)]\n",
      "friday: [(2, 17)]\n",
      "game: [(0, 42)]\n",
      "garden: [(4, 5), (4, 21)]\n",
      "geoff: [(1, 31)]\n",
      "gold: [(4, 34)]\n",
      "grand: [(0, 22)]\n",
      "great: [(4, 15)]\n",
      "greek: [(3, 0), (3, 18)]\n",
      "gut: [(1, 22)]\n",
      "hewitt: [(1, 18), (1, 19), (2, 0), (2, 6)]\n",
      "hold: [(5, 24)]\n",
      "home: [(2, 20)]\n",
      "honour: [(4, 3), (4, 11)]\n",
      "indoor: [(5, 2), (5, 8)]\n",
      "injuri: [(0, 37)]\n",
      "jame: [(5, 4)]\n",
      "jason: [(4, 4)]\n",
      "jimmi: [(5, 28)]\n",
      "katerina: [(3, 9)]\n",
      "kenteri: [(3, 8), (3, 29)]\n",
      "kept: [(2, 7)]\n",
      "kosta: [(3, 7)]\n",
      "larn: [(5, 17)]\n",
      "late: [(5, 25)]\n",
      "left: [(0, 38)]\n",
      "lewisfr: [(4, 27)]\n",
      "lindsay: [(0, 11)]\n",
      "list: [(4, 12)]\n",
      "lleyton: [(1, 17), (2, 5)]\n",
      "look: [(0, 33)]\n",
      "made: [(4, 7)]\n",
      "make: [(1, 26)]\n",
      "marat: [(2, 23)]\n",
      "mark: [(4, 26)]\n",
      "marlon: [(4, 24)]\n",
      "mbe: [(4, 8), (4, 29)]\n",
      "mcilroy: [(5, 0), (5, 5)]\n",
      "melbourn: [(1, 12)]\n",
      "metr: [(5, 32)]\n",
      "minut: [(5, 14)]\n",
      "miss: [(3, 15)]\n",
      "motor: [(5, 6)]\n",
      "new: [(4, 9)]\n",
      "nt: [(3, 3), (3, 32)]\n",
      "offici: [(1, 0), (1, 7)]\n",
      "olymp: [(4, 20)]\n",
      "one: [(5, 13)]\n",
      "open: [(0, 16), (1, 10), (2, 10)]\n",
      "organis: [(3, 13)]\n",
      "park: [(1, 13)]\n",
      "parliament: [(3, 26)]\n",
      "persuad: [(1, 25)]\n",
      "play: [(1, 15)]\n",
      "pollard: [(1, 32)]\n",
      "prerac: [(4, 31)]\n",
      "quit: [(0, 35)]\n",
      "race: [(3, 33), (5, 20)]\n",
      "reach: [(2, 3)]\n",
      "recoveri: [(0, 9)]\n",
      "reject: [(1, 33)]\n",
      "relay: [(4, 0), (4, 18)]\n",
      "remark: [(0, 8)]\n",
      "respond: [(1, 1)]\n",
      "rib: [(0, 36)]\n",
      "roddick: [(2, 16)]\n",
      "row: [(1, 3)]\n",
      "run: [(3, 4)]\n",
      "safin: [(2, 24)]\n",
      "said: [(1, 20)]\n",
      "say: [(3, 11), (4, 6)]\n",
      "second: [(0, 14), (2, 18), (5, 16)]\n",
      "semifin: [(2, 19)]\n",
      "serena: [(0, 5)]\n",
      "sevasti: [(3, 23)]\n",
      "seventh: [(0, 21)]\n",
      "sheffi: [(5, 10)]\n",
      "sinc: [(0, 26)]\n",
      "slam: [(0, 23)]\n",
      "sprinter: [(3, 1), (3, 6)]\n",
      "squad: [(4, 1), (4, 19)]\n",
      "stage: [(0, 7)]\n",
      "start: [(5, 21)]\n",
      "sunday: [(2, 25), (5, 11)]\n",
      "surfac: [(1, 16), (1, 27)]\n",
      "tenni: [(0, 1), (1, 5), (1, 29)]\n",
      "test: [(3, 17)]\n",
      "thanou: [(3, 10), (3, 30)]\n",
      "thrill: [(4, 2)]\n",
      "time: [(5, 12)]\n",
      "titl: [(0, 4), (0, 17), (0, 24), (2, 11), (5, 3), (5, 9)]\n",
      "told: [(3, 24)]\n",
      "top: [(1, 6)]\n",
      "treatment: [(0, 40)]\n",
      "tri: [(1, 24)]\n",
      "underlin: [(4, 13)]\n",
      "usa: [(4, 33)]\n",
      "vassilli: [(3, 22)]\n",
      "watkin: [(5, 29)]\n",
      "welshman: [(5, 27)]\n",
      "william: [(0, 0), (0, 6), (0, 32)]\n",
      "wimbledon: [(0, 27)]\n",
      "win: [(0, 13), (2, 14), (5, 1)]\n",
      "wo: [(3, 2), (3, 31)]\n",
      "xm: [(4, 17)]\n",
      "year: [(4, 10)]\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "\n",
    "doc_path = { \"docx\" : \"./BBC_Sport/docs\", \n",
    "             \"txt\" : \"./BBC Sport/text\"}\n",
    "\n",
    "# txt_path = \"./Assignment-1/BBC Sport/docs\"\n",
    "directory_path = './Assignment-1/BBC Sport/docs'\n",
    "\n",
    "print (f\"1. Read dataset\")\n",
    "documents = read_multiple_docx(doc_path)\n",
    "# print (documents)\n",
    "print (f\"2. Pre-processing Complete.\")\n",
    "\n",
    "# Create token stream\n",
    "token_stream = []\n",
    "for doc_id, (filename, tokens) in enumerate(documents.items()):\n",
    "    for pos, token in enumerate(tokens):\n",
    "        token_stream.append((token, doc_id, pos))\n",
    "\n",
    "# print (type(token_stream))\n",
    "# print(token_stream)\n",
    "\n",
    "# Construct positional index using SPIMI\n",
    "num_blocks = spimi_invert(token_stream)\n",
    "print (type(num_blocks))\n",
    "print (num_blocks)\n",
    "positional_index = merge_blocks(num_blocks)\n",
    "\n",
    "# Print positional index\n",
    "for term, postings in sorted(positional_index.items()):\n",
    "    print(f'{term}: {postings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afaeaafd-f9a2-456d-8e5d-ee2bf0dd8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect (ltup1, ltup2):\n",
    "    set1 = set(ltup1)\n",
    "    set2 = set(ltup2)\n",
    "    intersection = set1 & set2\n",
    "    return intersection\n",
    "    \n",
    "def union (ltup1, ltup2):\n",
    "    set1 = set(ltup1)\n",
    "    set2 = set(ltup2)\n",
    "    union = set1 | set2\n",
    "    return union\n",
    "\n",
    "# a complement b\n",
    "def a_not_in_b (ltup1, ltup2):\n",
    "    set1 = set(ltup1)\n",
    "    set2 = set(ltup2)\n",
    "    not_in_set1 = set2 - set1\n",
    "    return not_in_set1\n",
    "\n",
    "# b complement a \n",
    "def b_not_in_a (ltup1, ltup2):\n",
    "    set1 = set(ltup1)\n",
    "    set2 = set(ltup2)\n",
    "    not_in_set2 = set1 - set2\n",
    "    return not_in_set2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9469c7f-7106-479a-93de-b56f39392958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def intersect(postings1, postings2):\n",
    "#     \"\"\"Intersect two postings lists.\"\"\"\n",
    "#     i, j = 0, 0\n",
    "#     result = []\n",
    "#     while i < len(postings1) and j < len(postings2):\n",
    "#         if postings1[i] == postings2[j]:\n",
    "#             result.append(postings1[i])\n",
    "#             i += 1\n",
    "#             j += 1\n",
    "#         elif postings1[i] < postings2[j]:\n",
    "#             i += 1\n",
    "#         else:\n",
    "#             j += 1\n",
    "#     return result\n",
    "\n",
    "# def union(postings1, postings2):\n",
    "#     \"\"\"Union two postings lists.\"\"\"\n",
    "#     i, j = 0, 0\n",
    "#     result = []\n",
    "#     while i < len(postings1) and j < len(postings2):\n",
    "#         if postings1[i] == postings2[j]:\n",
    "#             result.append(postings1[i])\n",
    "#             i += 1\n",
    "#             j += 1\n",
    "#         elif postings1[i] < postings2[j]:\n",
    "#             result.append(postings1[i])\n",
    "#             i += 1\n",
    "#         else:\n",
    "#             result.append(postings2[j])\n",
    "#             j += 1\n",
    "#     result.extend(postings1[i:])\n",
    "#     result.extend(postings2[j:])\n",
    "#     return result\n",
    "\n",
    "# def difference(postings1, postings2):\n",
    "#     \"\"\"Difference between two postings lists.\"\"\"\n",
    "#     i, j = 0, 0\n",
    "#     result = []\n",
    "#     while i < len(postings1) and j < len(postings2):\n",
    "#         if postings1[i] == postings2[j]:\n",
    "#             i += 1\n",
    "#             j += 1\n",
    "#         elif postings1[i] < postings2[j]:\n",
    "#             result.append(postings1[i])\n",
    "#             i += 1\n",
    "#         else:\n",
    "#             j += 1\n",
    "#     result.extend(postings1[i:])\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56267316-6f81-486e-bc4d-bcce673aa208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89897f7-71d2-432a-9377-58b1d8f2a240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c72723e-e6fe-494b-8992-4a608dede281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "terms: ['will', 'and', 'title']\n",
      "operators : ['AND']\n",
      "postings_lists: {'titl': [(0, 4), (0, 17), (0, 24), (2, 11), (5, 3), (5, 9)]}\n",
      "Documents matching the query 'will AND title': [(0, 4), (0, 17), (0, 24), (2, 11), (5, 3), (5, 9)]\n",
      "\n",
      "terms: ['win', 'or', 'title']\n",
      "operators : ['OR']\n",
      "postings_lists: {'win': [(0, 13), (2, 14), (5, 1)], 'titl': [(0, 4), (0, 17), (0, 24), (2, 11), (5, 3), (5, 9)]}\n",
      "Documents matching the query 'win OR title': {(2, 14), (0, 4), (0, 13), (5, 1), (5, 3), (5, 9), (0, 24), (2, 11), (0, 17)}\n",
      "\n",
      "terms: ['win', 'or', 'hewitt']\n",
      "operators : ['OR']\n",
      "postings_lists: {'win': [(0, 13), (2, 14), (5, 1)], 'hewitt': [(1, 18), (1, 19), (2, 0), (2, 6)]}\n",
      "Documents matching the query 'win OR hewitt': {(2, 14), (1, 18), (2, 6), (0, 13), (1, 19), (2, 0), (5, 1)}\n",
      "\n",
      "terms: ['win', 'and', 'hewitt']\n",
      "operators : ['AND']\n",
      "postings_lists: {'win': [(0, 13), (2, 14), (5, 1)], 'hewitt': [(1, 18), (1, 19), (2, 0), (2, 6)]}\n",
      "Documents matching the query 'win AND hewitt': set()\n"
     ]
    }
   ],
   "source": [
    "def get_postings(keys, index):\n",
    "    \"\"\"Retrieve postings list for a term from the positional index.\"\"\"\n",
    "    extracted = {key: index[key] for key in keys if key in index}\n",
    "    return extracted\n",
    "\n",
    "    \n",
    "\n",
    "def evaluate_boolean_query(query, index):\n",
    "    \"\"\"\n",
    "    Evaluate a Boolean query on the positional index.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): Boolean query in CNF\n",
    "    index (dict): Positional index\n",
    "    \n",
    "    Returns:\n",
    "    list: List of document IDs satisfying the query\n",
    "    \"\"\"\n",
    "    # Split the query into terms and operators\n",
    "    terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    operators = re.findall(r'AND|OR|NOT', query.upper())\n",
    "    print (f\"terms: {terms}\")\n",
    "    print (f\"operators : {operators}\")\n",
    "\n",
    "    # Preprocess terms\n",
    "    terms = [stemmer.stem(term) for term in terms if term not in stop_words]\n",
    "    \n",
    "    # Retrieve postings lists for each term\n",
    "    postings_lists = get_postings(terms, index)\n",
    "    print (f\"postings_lists: {postings_lists}\")  \n",
    "        \n",
    "    # Evaluate the query\n",
    "    if not operators:        \n",
    "        return postings_lists.get(terms[0], [])\n",
    "\n",
    "    result = postings_lists[terms[0]]\n",
    "    i = 1\n",
    "\n",
    "    while i < len(terms):\n",
    "        operator = operators[i - 1]\n",
    "        if operator == 'AND':\n",
    "            # print (f\"param1:{result}, param2:{postings_lists[terms[i]]}\")\n",
    "            result = intersect(result, postings_lists[terms[i]])                        \n",
    "        elif operator == 'OR':\n",
    "            result = union(result, postings_lists[terms[i]])            \n",
    "        elif operator == 'NOT':\n",
    "            result = complement(result, postings_lists[terms[i]])            \n",
    "        i += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# Demo\n",
    "print ()\n",
    "query = \"will AND title\"\n",
    "result = evaluate_boolean_query(query, positional_index)\n",
    "print(f\"Documents matching the query '{query}': {result}\")\n",
    "print ()\n",
    "query = \"win OR title\"\n",
    "result = evaluate_boolean_query(query, positional_index)\n",
    "print(f\"Documents matching the query '{query}': {result}\")\n",
    "print ()\n",
    "query = \"win OR hewitt\"\n",
    "result = evaluate_boolean_query(query, positional_index)\n",
    "print(f\"Documents matching the query '{query}': {result}\")\n",
    "print()\n",
    "query = \"win AND hewitt\"\n",
    "result = evaluate_boolean_query(query, positional_index)\n",
    "print(f\"Documents matching the query '{query}': {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ba479-45b9-4673-9603-0ecce5655958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee77a0-032b-4dde-a70d-c260ea9b1a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57353846-21bc-42ad-9e7c-53de0de5848d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081eb43f-d33b-4294-8456-264fd34a28aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4ebff-119f-4d6e-96ff-328d992e5fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
