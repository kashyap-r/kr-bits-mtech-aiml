{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "\n",
    "ALLOCBLOCKSIZE = 30000\n",
    "BASEINDEXPATH = 'index/'\n",
    "BASESOURCEPATH = 'source/'\n",
    "\n",
    "def download():\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Normalize text\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
    "\n",
    "\n",
    "    words = word_tokenize(text) #tokenize\n",
    "    tokens = [w for w in words if not w.lower() in stop_words] #stopword removal\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeblock(dictionary):\n",
    "    sorted_terms = sorted(dictionary)\n",
    "    block_file = BASEINDEXPATH + ''.join(random.choices(string.ascii_lowercase, k=8))+'.block'\n",
    "    with open(block_file, 'w+') as file:\n",
    "        for term in sorted_terms:\n",
    "            file.write('%s %s\\n' %(term, json.dumps(dictionary[term],separators=(\",\", \":\"))))\n",
    "            \n",
    "    print(f\"block file saved - {block_file}\")\n",
    "    return block_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spimiinvert(documents):\n",
    "    dictionary = {}\n",
    "    block_files = []\n",
    "    for index, docId in enumerate(documents):\n",
    "        for index, term in enumerate(documents[docId]):\n",
    "            if term not in dictionary:\n",
    "                dictionary[term] = {docId: [index]}\n",
    "            else:\n",
    "                if docId not in dictionary[term]:\n",
    "                    dictionary[term][docId] = [index]\n",
    "                else:\n",
    "                    dictionary[term][docId].append(index)\n",
    "        if docId == len(documents) - 1 or sys.getsizeof(dictionary) > int(ALLOCBLOCKSIZE):\n",
    "            print(f'dictionary size {sys.getsizeof(dictionary)}. Saving to disk')\n",
    "            block_files.append(writeblock(dictionary))\n",
    "            dictionary = {}\n",
    "    return block_files   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeblock(block_files):\n",
    "    index_file = BASEINDEXPATH + 'inverted_index.txt'\n",
    "    final_index = {}\n",
    "    for block_file in block_files:            \n",
    "        with open(index_file, 'w+') as file:\n",
    "            for line in file:\n",
    "                currentkey, currentvalue = line.strip().split(' ')\n",
    "                final_index[currentkey] = json.load(currentvalue)\n",
    "\n",
    "            with open(block_file, 'r') as block:\n",
    "                for entry in block:\n",
    "                    key , newvalue = entry.strip().split(' ')\n",
    "                    newvalue = json.loads(newvalue)\n",
    "                    if key not in final_index:\n",
    "                        final_index[key]= newvalue\n",
    "                    else:\n",
    "                        final_index[key].update(newvalue)\n",
    "            \n",
    "            sorted_index = sorted(final_index)\n",
    "            if(block_files.index(block_file) == len(block_files) - 1):\n",
    "                # write final merged index with frequency\n",
    "                 for term in sorted_index:\n",
    "                    freqmap = {}\n",
    "                    total = 0\n",
    "                    for doc in final_index[term]:\n",
    "                        count = len(final_index[term][doc])\n",
    "                        total = total + count\n",
    "                        freqmap[f\"{doc}_{count}\"] =  final_index[term][doc]\n",
    "                    file.write('%s_%s %s\\n' % (term, total, json.dumps(freqmap,separators=(\",\", \":\"))))\n",
    "            else:\n",
    "                # write intermediate merged index    \n",
    "                for term in sorted_index:\n",
    "                    file.write('%s %s\\n' % (term, json.dumps(final_index[term],separators=(\",\", \":\"))))\n",
    "    \n",
    "    return index_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\usha_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usha_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\usha_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\usha_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory doc1.docx.\n",
      "Current directory doc2.docx.\n",
      "Current directory doc3.docx.\n",
      "Current directory doc4.docx.\n",
      "Current directory doc5.docx.\n",
      "Current directory doc6.docx.\n",
      "Index file created - ./Output/inverted_index.txt\n"
     ]
    }
   ],
   "source": [
    "docx_path = \"./BBC Sport/docs\"\n",
    "txt_path = \"./BBC Sport/text\"\n",
    "\n",
    "BASESOURCEPATH = \"./BBC_Sport/docs\"\n",
    "BASEINDEXPATH = \"./Output/\"\n",
    "\n",
    "def create_index():\n",
    "    documents = {}\n",
    "    index = 0\n",
    "\n",
    "    for index, p in enumerate(Path(BASESOURCEPATH).iterdir()):\n",
    "        print(f\"Current directory {p.name}.\")\n",
    "        for i in p.glob('*.txt'):\n",
    "            print(f\"\\t Found file {i.name}. Preprocessing file\")\n",
    "            documents[index] = preprocess(i.read_text())\n",
    "            index = index + 1\n",
    "\n",
    "        for i in p.glob('*.docx'):\n",
    "            print(f\"\\t Found file {i.name}. Preprocessing file\")\n",
    "            doc = Document(i)\n",
    "            documents[index] = preprocess(\" \".join([p.text for p in doc.paragraphs]))\n",
    "            index = index + 1\n",
    "\n",
    "    \n",
    "    \n",
    "    blocks = spimiinvert(documents)\n",
    "    index_file = mergeblock(blocks)\n",
    "    print(f\"Index file created - {index_file}\")\n",
    "\n",
    "create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory doc1.docx.\n",
      "Current directory doc2.docx.\n",
      "Current directory doc3.docx.\n",
      "Current directory doc4.docx.\n",
      "Current directory doc5.docx.\n",
      "Current directory doc6.docx.\n",
      "{}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
