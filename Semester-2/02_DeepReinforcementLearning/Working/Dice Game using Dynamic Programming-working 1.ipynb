{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4499e481-85af-4710-a9dd-412265464838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a reinforcement learning agent for the Dice game using dynamic programming in Python involves several steps. Below, I'll outline a basic structure and provide code snippets for each part:\n",
    "# Step 1: Initialize Environment Constants\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "GOAL = 100  # Target score to reach\n",
    "NUM_SIDES = 6  # Number of sides on the die\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe95a716-2be2-4251-b0cf-537faf873da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Functions for Game Mechanics\n",
    "\n",
    "def roll_die():\n",
    "    \"\"\" Simulate rolling a 6-sided die. \"\"\"\n",
    "    return np.random.randint(1, NUM_SIDES + 1)\n",
    "\n",
    "def take_action(state, action):\n",
    "    \"\"\" Execute the action (roll or stop) and return the next state and reward. \"\"\"\n",
    "    if action == 'roll':\n",
    "        roll = roll_die()\n",
    "        if roll == 1:\n",
    "            return 0, -1  # Lose all points accumulated in the turn\n",
    "        else:\n",
    "            return state + roll, roll  # Add roll to the current score\n",
    "    elif action == 'stop':\n",
    "        return GOAL, 0  # Stop and keep the current score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319b6ea-056a-43fe-9e72-5d95f025677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize Value Function and Policy\n",
    "\n",
    "V = np.zeros(GOAL + 1)  # Value function V(s)\n",
    "policy = np.zeros(GOAL + 1, dtype=np.object)  # Policy œÄ(s), where each element can be 'roll' or 'stop'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc15b56-01d9-459c-b915-933ac24d5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Dynamic Programming - Value Iteration\n",
    "\n",
    "def value_iteration():\n",
    "    theta = 1e-5  # Convergence threshold\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(1, GOAL):  # Iterate over all states from 1 to GOAL-1\n",
    "            v = V[s]\n",
    "            V[s] = max(q_value(s, 'roll'), q_value(s, 'stop'))\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "def q_value(state, action):\n",
    "    if action == 'roll':\n",
    "        return roll_action_value(state)\n",
    "    elif action == 'stop':\n",
    "        return state  # Stopping maintains the current score\n",
    "\n",
    "def roll_action_value(state):\n",
    "    expected_value = 0\n",
    "    for roll in range(2, NUM_SIDES + 1):\n",
    "        next_state, reward = take_action(state, 'roll')\n",
    "        expected_value += (1 / NUM_SIDES) * (reward + V[next_state])\n",
    "    return expected_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f060a72-cb40-4a6f-96d7-76d3f616ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Policy Improvement\n",
    "\n",
    "def policy_improvement():\n",
    "    for s in range(1, GOAL):\n",
    "        if q_value(s, 'roll') >= q_value(s, 'stop'):\n",
    "            policy[s] = 'roll'\n",
    "        else:\n",
    "            policy[s] = 'stop'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33410523-482e-43e9-b8c9-744667f8a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Training Loop\n",
    "\n",
    "def train_agent(num_iterations):\n",
    "    for _ in range(num_iterations):\n",
    "        value_iteration()\n",
    "        policy_improvement()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fdb32-d082-4c89-809e-5e5a93fab4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Testing and Evaluation\n",
    "\n",
    "def test_agent():\n",
    "    state = 0  # Starting state\n",
    "    while state != GOAL:\n",
    "        action = policy[state]\n",
    "        state, _ = take_action(state, action)\n",
    "    return state == GOAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004897cc-ec03-48b3-90bd-b99bd587110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Putting It All Together\n",
    "if __name__ == '__main__':\n",
    "    train_agent(num_iterations=1000)\n",
    "    success_count = sum(test_agent() for _ in range(1000))\n",
    "    print(f\"Probability of reaching exactly 100 points: {success_count / 1000}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f6cb7-7c6b-4d5f-9ec6-8a86eb0c076e",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Value Iteration: Computes the value function ùëâ(ùë†) or V(s) iteratively until convergence.\n",
    "Policy Improvement: Updates the policy based on the current value function.\n",
    "Training Loop: Iterates over a specified number of training iterations to improve the agent's policy.\n",
    "Testing: Evaluates the learned policy by running multiple simulations and calculating the probability of reaching exactly 100 points.\n",
    "\n",
    "This code provides a basic framework. Depending on your specific requirements (such as fine-tuning convergence criteria, handling edge cases, or optimizing for performance), you may need to adjust and expand upon these snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35a480-833f-49bb-8788-32fa6b069fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa025c8-d4aa-40ef-aa7e-e50652ca0b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22279a0d-8411-4134-b8ab-acc25b5f9fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
